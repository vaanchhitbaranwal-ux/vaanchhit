{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaanchhitbaranwal-ux/vaanchhit/blob/main/Routing_Chains_for_LCEL(LangChain_Expression_Language)_Langchain_LCEL_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import argparse\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "from typing import AsyncIterator, Dict, List, Any\n",
        "\n",
        "# Rich Imports\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "from rich.panel import Panel\n",
        "from rich.live import Live\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# --- UX Utilities ---\n",
        "console = Console()\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = \"gemma3:12b\"  # Fallback to gemma2:latest if needed\n",
        "EMBEDDING_MODEL = \"nomic-embed-text\" # Standard decent local embedding model, usually avail with Ollama users\n",
        "\n",
        "def get_llm():\n",
        "    \"\"\"Configures the local Ollama LLM.\"\"\"\n",
        "    return ChatOllama(\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0.2,\n",
        "        num_ctx=8192,\n",
        "    )\n",
        "\n",
        "def get_embeddings():\n",
        "    \"\"\"Configures local embeddings.\"\"\"\n",
        "    return OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
        "\n",
        "# --- Part 1: Branching & Routing ---\n",
        "\n",
        "def run_routing_demo(query: str):\n",
        "    console.print(Panel(f\"[bold blue]Running Routing Demo[/bold blue]\\nQuery: [italic]{query}[/italic]\"))\n",
        "    llm = get_llm()\n",
        "\n",
        "    # 1. Define distinct chains\n",
        "\n",
        "    # Code Chain\n",
        "    code_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a senior Python engineer. Answer in code blocks.\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ])\n",
        "    code_chain = code_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Data Chain\n",
        "    data_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a data scientist. Focus on analysis and metrics.\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ])\n",
        "    data_chain = data_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # General Chain\n",
        "    general_prompt = ChatPromptTemplate.from_template(\"You are a helpful assistant.\\nQuestion: {query}\")\n",
        "    general_chain = general_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # 2. Intent Classifier\n",
        "    classifier_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Classify the user intent into exactly one of these labels: 'code', 'data', 'general'. Do not output anything else.\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ])\n",
        "\n",
        "    # Simple parser to clean up response just in case\n",
        "    def parse_intent(ai_msg):\n",
        "        text = ai_msg.content.strip().lower()\n",
        "        if \"code\" in text: return \"code\"\n",
        "        if \"data\" in text: return \"data\"\n",
        "        return \"general\"\n",
        "\n",
        "    classifier_chain = classifier_prompt | llm | RunnableLambda(parse_intent)\n",
        "\n",
        "    # 3. Routing Logic\n",
        "\n",
        "    chain_with_intent = RunnableParallel({\n",
        "        \"intent\": classifier_chain,\n",
        "        \"query\": RunnablePassthrough()\n",
        "    })\n",
        "\n",
        "    routing_chain = chain_with_intent | RunnableBranch(\n",
        "        (lambda x: x[\"intent\"] == \"code\", RunnableLambda(lambda x: console.print(f\"[bold green][Router] Detected 'code'[/bold green]\") or x) | RunnableLambda(lambda x: x[\"query\"]) | code_chain),\n",
        "        (lambda x: x[\"intent\"] == \"data\", RunnableLambda(lambda x: console.print(f\"[bold cyan][Router] Detected 'data'[/bold cyan]\") or x) | RunnableLambda(lambda x: x[\"query\"]) | data_chain),\n",
        "        RunnableLambda(lambda x: console.print(f\"[bold yellow][Router] Defaulting to 'general' (intent={x['intent']})[/bold yellow]\") or x) | RunnableLambda(lambda x: x[\"query\"]) | general_chain\n",
        "    )\n",
        "\n",
        "    # Invoke\n",
        "    with console.status(\"[bold green]Routing and Generating Response...[/bold green]\", spinner=\"dots\"):\n",
        "        result = routing_chain.invoke(query)\n",
        "\n",
        "    console.print(\"\\n[bold]Final Output:[/bold]\")\n",
        "    console.print(Markdown(result))\n",
        "    return result\n",
        "\n",
        "\n",
        "# --- Part 2: Parallel Fan-out (Multi-Retriever RAG) ---\n",
        "\n",
        "def create_retriever(name: str, texts: List[str]):\n",
        "    \"\"\"Helper to create an in-memory FAISS retriever from text.\"\"\"\n",
        "    console.print(f\"Building index for [bold]{name}[/bold]...\")\n",
        "    embeddings = get_embeddings()\n",
        "    docs = [Document(page_content=t, metadata={\"source\": name}) for t in texts]\n",
        "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "    return vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "def run_parallel_rag_demo(query: str):\n",
        "    console.print(Panel(f\"[bold blue]Running Parallel RAG Demo[/bold blue]\\nQuery: [italic]{query}[/italic]\"))\n",
        "\n",
        "    # 1. Setup Data\n",
        "    langchain_texts = [\n",
        "        \"LangChain is a framework for developing applications powered by language models.\",\n",
        "        \"LCEL (LangChain Expression Language) provides a declarative way to compose chains.\",\n",
        "        \"Runnables are the building blocks of LCEL.\"\n",
        "    ]\n",
        "    ollama_texts = [\n",
        "        \"Ollama allows you to run open-source large language models globally.\",\n",
        "        \"Gemma is a family of lightweight, state-of-the-art open models built by Google.\",\n",
        "        \"You can use 'ollama pull gemma3:12b' to get the model.\"\n",
        "    ]\n",
        "    misc_texts = [\n",
        "        \"Python 3.10 introduced structural pattern matching.\",\n",
        "        \"Asyncio is a library to write concurrent code using the async/await syntax.\",\n",
        "        \"Decorators in Python are a very powerful tool.\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        with console.status(\"[bold blue]indexing Documents...[/bold blue]\"):\n",
        "            retriever_lc = create_retriever(\"langchain\", langchain_texts)\n",
        "            retriever_ollama = create_retriever(\"ollama\", ollama_texts)\n",
        "            retriever_misc = create_retriever(\"misc\", misc_texts)\n",
        "    except Exception as e:\n",
        "        console.print(f\"[bold red]Error creating retrievers:[/bold red] {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Parallel Retrieval\n",
        "    parallel_retrievers = RunnableParallel({\n",
        "        \"lc_docs\": retriever_lc,\n",
        "        \"ollama_docs\": retriever_ollama,\n",
        "        \"misc_docs\": retriever_misc,\n",
        "    })\n",
        "\n",
        "    # 3. Merge Step\n",
        "    def merge_docs(inputs: dict) -> List[Document]:\n",
        "        merged = []\n",
        "        console.print(\"[bold purple][Merger][/bold purple] Retrieving from sources:\")\n",
        "        for key, docs in inputs.items():\n",
        "            console.print(f\"  - {key}: {len(docs)} docs found\")\n",
        "            merged.extend(docs)\n",
        "        return merged\n",
        "\n",
        "    # 4. RAG Chain\n",
        "    llm = get_llm()\n",
        "    rag_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful technical assistant. Answer strictly based on the context provided.\"),\n",
        "        (\"human\", \"Question: {query}\\n\\nContext:\\n{context}\")\n",
        "    ])\n",
        "\n",
        "    def format_docs(docs: List[Document]) -> str:\n",
        "        return \"\\n\\n\".join([f\"[{d.metadata['source']}] {d.page_content}\" for d in docs])\n",
        "\n",
        "    rag_chain = (\n",
        "        {\n",
        "            \"query\": RunnablePassthrough(),\n",
        "            \"context\": parallel_retrievers | RunnableLambda(merge_docs) | RunnableLambda(format_docs)\n",
        "        }\n",
        "        | rag_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Invoke\n",
        "    with console.status(\"[bold green]Parallel Retrieval & Synthesis in progress...[/bold green]\", spinner=\"dots\"):\n",
        "        result = rag_chain.invoke(query)\n",
        "\n",
        "    console.print(\"\\n[bold]Final Output:[/bold]\")\n",
        "    console.print(Markdown(result))\n",
        "    return result\n",
        "\n",
        "\n",
        "# --- Part 3: Streaming Middleware ---\n",
        "\n",
        "SENSITIVE_PATTERN = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\")\n",
        "\n",
        "async def middleware_stream(iterable: AsyncIterator[Any]) -> AsyncIterator[str]:\n",
        "    \"\"\"Async generator that acts as middleware with buffering to handle split tokens.\"\"\"\n",
        "    buffer = \"\"\n",
        "\n",
        "    async for chunk in iterable:\n",
        "        # 1. Extract text content\n",
        "        text = chunk.content if hasattr(chunk, \"content\") else str(chunk)\n",
        "        buffer += text\n",
        "\n",
        "        # 2. Check for separators to flush safe parts\n",
        "        # If the buffer ends with part of a potential email, we keep it.\n",
        "        # Simple heuristic: Split by space or newline. Process all but the last potentially incomplete word.\n",
        "\n",
        "        if \" \" in buffer or \"\\n\" in buffer:\n",
        "            # Find the last separator\n",
        "            last_space = max(buffer.rfind(\" \"), buffer.rfind(\"\\n\"))\n",
        "\n",
        "            # Extract safe chunk, keep remainder\n",
        "            to_process = buffer[:last_space+1] # Include the separator\n",
        "            buffer = buffer[last_space+1:]\n",
        "\n",
        "            # Redact in the safe chunk\n",
        "            safe_chunk = SENSITIVE_PATTERN.sub(\"[REDACTED_EMAIL]\", to_process)\n",
        "\n",
        "            # 3. Throttle (simulated per 'flush')\n",
        "            await asyncio.sleep(0.02)\n",
        "\n",
        "            yield safe_chunk\n",
        "\n",
        "    # Flush remaining buffer at the end\n",
        "    if buffer:\n",
        "        safe_chunk = SENSITIVE_PATTERN.sub(\"[REDACTED_EMAIL]\", buffer)\n",
        "        yield safe_chunk\n",
        "\n",
        "async def run_stream_middleware_demo(query: str):\n",
        "    console.print(Panel(f\"[bold blue]Running Streaming Middleware Demo[/bold blue]\\nQuery: [italic]{query}[/italic]\"))\n",
        "\n",
        "    llm = get_llm()\n",
        "    base_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a verbose assistant. If I ask about personal info, invent some fake emails.\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ])\n",
        "\n",
        "    # Base chain\n",
        "    chain = base_prompt | llm\n",
        "\n",
        "    console.print(\"\\n[bold]Streaming Output (with Middleware):[/bold]\\n\")\n",
        "\n",
        "    with console.status(\"[bold green]Initializing Stream...[/bold green]\"):\n",
        "        raw_iterator = chain.astream(query)\n",
        "        # Fake init delay to show spinner\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    # For streaming, we construct the text incrementally\n",
        "    # Rich doesn't support streaming Markdown updates easily in Live without flickering or full redraws.\n",
        "    # So we'll print raw text for the stream to appear 'streaming', then render final Markdown if we wanted.\n",
        "    # Or just stream plain text. The user asked for better markdown handling generally, for streaming plain text is usually clearer.\n",
        "    # However, we can use Live display to update a Markdown object if we buffer it. Let's try that for \"wow\" factor.\n",
        "\n",
        "    full_response = \"\"\n",
        "    with Live(Markdown(\"\"), refresh_per_second=10, console=console) as live:\n",
        "        async for chunk in middleware_stream(raw_iterator):\n",
        "            full_response += chunk\n",
        "            live.update(Markdown(full_response))\n",
        "\n",
        "    console.print(\"\\n\\n[bold green][Stream Complete][/bold green]\")\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"LCEL Advanced Capabilities Demo\")\n",
        "    parser.add_argument(\"mode\", choices=[\"routing\", \"parallel_rag\", \"stream_middleware\"], help=\"Demo mode to run\")\n",
        "    parser.add_argument(\"--query\", type=str, help=\"Input query\", default=\"\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Defaults if no query provided\n",
        "    defaults = {\n",
        "        \"routing\": \"Write a Python function to compute fibonacci.\",\n",
        "        \"parallel_rag\": \"What is LCEL and how do I download gemma?\",\n",
        "        \"stream_middleware\": \"My email is test@example.com, please repeat it back to me.\"\n",
        "    }\n",
        "\n",
        "    query = args.query if args.query else defaults[args.mode]\n",
        "\n",
        "    if args.mode == \"routing\":\n",
        "        run_routing_demo(query)\n",
        "    elif args.mode == \"parallel_rag\":\n",
        "        run_parallel_rag_demo(query)\n",
        "    elif args.mode == \"stream_middleware\":\n",
        "        asyncio.run(run_stream_middleware_demo(query))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "WNpcp6e4Mmc7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}