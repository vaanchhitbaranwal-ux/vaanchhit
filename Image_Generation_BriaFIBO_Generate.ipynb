{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaanchhitbaranwal-ux/vaanchhit/blob/main/Image_Generation_BriaFIBO_Generate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from src.fibo_inference.inference import (\n",
        "    create_pipeline,\n",
        "    resolve_structured_prompt,\n",
        "    run,\n",
        ")\n",
        "from src.fibo_inference.parse_caption import clean_json, prepare_clean_caption\n",
        "from src.fibo_inference.vlm.common import DEFAULT_SAMPLING, DEFAULT_STOP_SEQUENCES\n",
        "\n",
        "RESOLUTIONS_WH = [\n",
        "    \"832 1248\",\n",
        "    \"896 1152\",\n",
        "    \"960 1088\",\n",
        "    \"1024 1024\",\n",
        "    \"1088 960\",\n",
        "    \"1152 896\",\n",
        "    \"1216 832\",\n",
        "    \"1280 800\",\n",
        "    \"1344 768\",\n",
        "]\n",
        "\n",
        "DEFAULT_STEPS = 50\n",
        "DEFAULT_OUTPUT_PATH = \"output/generated.png\"\n",
        "\n",
        "\n",
        "def get_default_negative_prompt(existing_json: dict) -> str:\n",
        "    negative_prompt = \"\"\n",
        "    style_medium = existing_json.get(\"style_medium\", \"\").lower()\n",
        "    if style_medium in [\"photograph\", \"photography\", \"photo\"]:\n",
        "        negative_prompt = \"\"\"{'style_medium':'digital illustration','artistic_style':'non-realistic'}\"\"\"\n",
        "    return negative_prompt\n",
        "\n",
        "\n",
        "def load_default_prompt() -> dict:\n",
        "    \"\"\"Load and normalise the default caption JSON used by the Gradio demo.\"\"\"\n",
        "    default_path = Path(\"default_json_caption.json\")\n",
        "    with default_path.open() as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    data[\"pickascore\"] = 1.0\n",
        "    data[\"aesthetic_score\"] = 10.0\n",
        "    cleaned = prepare_clean_caption(data)\n",
        "    return json.loads(cleaned)\n",
        "\n",
        "\n",
        "def parse_resolution(raw_value: str) -> tuple[int, int]:\n",
        "    \"\"\"Parse resolution in the form 'WIDTH HEIGHT'.\"\"\"\n",
        "    normalised = raw_value.replace(\",\", \" \").replace(\"x\", \" \")\n",
        "    parts = [part for part in normalised.split() if part]\n",
        "    if len(parts) != 2:\n",
        "        raise SystemExit(\"Resolution must contain exactly two integers, e.g. '1024 1024'.\")\n",
        "\n",
        "    try:\n",
        "        width, height = (int(parts[0]), int(parts[1]))\n",
        "    except ValueError as exc:\n",
        "        raise SystemExit(\"Resolution values must be integers.\") from exc\n",
        "\n",
        "    if width <= 0 or height <= 0:\n",
        "        raise SystemExit(\"Resolution values must be positive.\")\n",
        "\n",
        "    return width, height\n",
        "\n",
        "\n",
        "def build_parser() -> argparse.ArgumentParser:\n",
        "    \"\"\"Configure the CLI parser.\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Generate images with the FIBO model.\")\n",
        "    parser.add_argument(\"--pipeline-name\", type=str, default=\"briaai/FIBO\", help=\"Pipeline name to use.\")\n",
        "    parser.add_argument(\"--vlm-model\", type=str, default=\"briaai/FIBO-vlm\", help=\"VLM model to use.\")\n",
        "    parser.add_argument(\"--model-mode\", choices=[\"local\", \"gemini\"], default=\"gemini\", help=\"Model mode to use.\")\n",
        "    parser.add_argument(\n",
        "        \"--negative-prompt\",\n",
        "        default=\"\",\n",
        "        help=\"Negative prompt\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--seed\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"Random seed. Use -1 for a random seed on each run.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--resolution\",\n",
        "        default=\"1024 1024\",\n",
        "        help=\"Output resolution as 'WIDTH HEIGHT'.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--guidance-scale\",\n",
        "        type=float,\n",
        "        default=5.0,\n",
        "        help=\"Classifier-free guidance scale.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--steps\",\n",
        "        type=int,\n",
        "        default=DEFAULT_STEPS,\n",
        "        dest=\"num_steps\",\n",
        "        help=\"Number of inference steps.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--prompt\",\n",
        "        help=\"Short natural-language prompt.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--structured-prompt\",\n",
        "        help=\"Existing structured prompt.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--image-path\",\n",
        "        help=\"Path to the image to be used by the VLM.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--temperature\",\n",
        "        type=float,\n",
        "        default=DEFAULT_SAMPLING.temperature,\n",
        "        help=\"Override temperature for VLM prompt generation (optional).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--top-p\",\n",
        "        type=float,\n",
        "        default=DEFAULT_SAMPLING.top_p,\n",
        "        help=\"Override top-p for VLM prompt generation (optional).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max-tokens\",\n",
        "        type=int,\n",
        "        default=DEFAULT_SAMPLING.max_tokens,\n",
        "        help=\"Override max tokens for VLM prompt generation (optional).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--stop-sequence\",\n",
        "        action=\"append\",\n",
        "        dest=\"stop_sequences\",\n",
        "        default=DEFAULT_STOP_SEQUENCES,\n",
        "        help=(\"Custom stop sequence for VLM prompt generation (repeat for multiple values).\"),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        default=DEFAULT_OUTPUT_PATH,\n",
        "        help=f\"Output image path (default: {DEFAULT_OUTPUT_PATH}).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--enable-teacache\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Enable TeaCache for faster inference with minimal quality loss.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--teacache-threshold\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        help=\"TeaCache threshold (0.6-1.0). Higher = faster but potentially lower quality. Default: 1.0\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lora-path\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"Path to the LoRA checkpoint.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fibo-lite\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Use FIBO-lite pipeline.\",\n",
        "    )\n",
        "    return parser\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def main():\n",
        "    default_prompt = load_default_prompt()\n",
        "    parser = build_parser()\n",
        "    args = parser.parse_args()\n",
        "    if args.structured_prompt is None and args.prompt is None and args.image_path is None:\n",
        "        print(\"Generating with default prompt\")\n",
        "        args.structured_prompt = json.dumps(default_prompt)\n",
        "    if args.fibo_lite:\n",
        "        args.pipeline_name = \"briaai/FIBO-lite\"\n",
        "    if args.model_mode == \"gemini\":\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "        if api_key is None:\n",
        "            raise SystemExit(\n",
        "                \"GOOGLE_API_KEY is not set, please set it in the environment variables or switch to local mode\"\n",
        "            )\n",
        "    if args.num_steps <= 0:\n",
        "        raise SystemExit(\"--steps must be a positive integer.\")\n",
        "\n",
        "    if args.seed >= 0:\n",
        "        random.seed(args.seed)\n",
        "        np.random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "\n",
        "    width, height = parse_resolution(args.resolution)\n",
        "    if f\"{width} {height}\" not in RESOLUTIONS_WH:\n",
        "        print(f\"Note: {width}x{height} is outside the preset resolutions used by the original demo.\")\n",
        "\n",
        "    assert torch.cuda.is_available(), \"CUDA not available\"\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    if args.structured_prompt is not None and args.prompt is None and args.image_path is None:\n",
        "        # json input and no other input -- skip VLM\n",
        "        if args.structured_prompt.endswith(\".json\"):\n",
        "            json_prompt = json.loads(open(args.structured_prompt).read())\n",
        "        else:\n",
        "            json_prompt = json.loads(args.structured_prompt)\n",
        "    else:\n",
        "        json_prompt = resolve_structured_prompt(\n",
        "            model_mode=args.model_mode,\n",
        "            device=\"cuda\",\n",
        "            vlm_model=args.vlm_model,\n",
        "            image_path=args.image_path,\n",
        "            prompt=args.prompt,\n",
        "            structured_prompt=args.structured_prompt,\n",
        "            stop_sequences=args.stop_sequences,\n",
        "            temperature=args.temperature,\n",
        "            top_p=args.top_p,\n",
        "            max_tokens=args.max_tokens,\n",
        "        )\n",
        "    elapsed = time.perf_counter() - start_time\n",
        "    print(f\"VLM prompt generation time: {elapsed:.2f} seconds\")\n",
        "\n",
        "    prompt_payload = clean_json(json_prompt)\n",
        "    negative_payload = args.negative_prompt\n",
        "    if negative_payload == \"\":\n",
        "        negative_payload = get_default_negative_prompt(json.loads(prompt_payload))\n",
        "\n",
        "    pipeline = create_pipeline(pipeline_name=args.pipeline_name, device=\"cuda\", lora_path=args.lora_path)\n",
        "\n",
        "    if args.enable_teacache:\n",
        "        print(f\"Enabling TeaCache with threshold={args.teacache_threshold}\")\n",
        "        pipeline.enable_teacache(num_inference_steps=args.num_steps, rel_l1_thresh=args.teacache_threshold)\n",
        "\n",
        "    if isinstance(json_prompt, dict) and \"short_description\" in json_prompt:\n",
        "        print(f\"short_description: {json_prompt['short_description']}\")\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    image = run(\n",
        "        pipeline=pipeline,\n",
        "        prompt_payload=prompt_payload,\n",
        "        negative_payload=negative_payload,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        seed=args.seed,\n",
        "        num_steps=args.num_steps,\n",
        "        guidance_scale=args.guidance_scale,\n",
        "    )\n",
        "    elapsed = time.perf_counter() - start_time\n",
        "\n",
        "    output_path = Path(args.output)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    image.save(output_path)\n",
        "    # dump json_prompt to a file\n",
        "    with open(output_path.with_suffix(\".json\"), \"w\") as f:\n",
        "        json.dump(json.loads(prompt_payload), f, indent=2)\n",
        "    print(f\"Generation time: {elapsed:.2f} seconds\")\n",
        "    print(f\"Saved image to {output_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Lo19rN0e1OP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}