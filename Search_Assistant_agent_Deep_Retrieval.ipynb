{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaanchhitbaranwal-ux/vaanchhit/blob/main/Search_Assistant_agent_Deep_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import argparse\n",
        "import re\n",
        "import time\n",
        "\n",
        "INSTRUCTION = \"\"\"\n",
        "You are a query rewriting expert. Your task is to create query terms for user query to find relevant literature in a Wikipedia corpus using BM25.\n",
        "\"\"\"\n",
        "\n",
        "def format_prompt(user_query: str) -> str:\n",
        "    \"\"\"Format the prompt for the model using the same template as make_prefix.\"\"\"\n",
        "    input_str = \"\"\"<|im_start|>system\\nYou are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.<|im_end|>\\n<|im_start|>user\\n\"\"\" + INSTRUCTION\n",
        "    input_str += \"\"\"\\nShow your work in <think> </think> tags. Your final response must be in JSON format within <answer> </answer> tags. For example,\n",
        "<think>\n",
        "[thinking process]\n",
        "</think>\n",
        "<answer>\n",
        "{\n",
        "    \"query\": \"....\"\n",
        "}\n",
        "</answer>.\n",
        "Note: The query should use Boolean operators (AND, OR) and parentheses for grouping terms appropriately.\n",
        "\n",
        "Here's the user query:\n",
        "\"\"\"\n",
        "    input_str += user_query + \"\"\"\n",
        "Assistant: Let me rewrite the query with reasoning.\n",
        "<think>\n",
        "\"\"\"\n",
        "\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"},\n",
        "        {\"role\": \"user\", \"content\": input_str}\n",
        "    ]\n",
        "\n",
        "def extract_query(response_text: str) -> str:\n",
        "    \"\"\"Extract the rewritten query from the model's response.\"\"\"\n",
        "    try:\n",
        "        # Find the last occurrence of <answer>...</answer>\n",
        "        if \"<answer>\" not in response_text:\n",
        "            response_text = \"<answer>\" + response_text\n",
        "        if \"</answer>\" not in response_text:\n",
        "            response_text = response_text + \"</answer>\"\n",
        "        answer_pattern = r'<answer>(.*?)</answer>'\n",
        "        matches = re.findall(answer_pattern, response_text, re.DOTALL)\n",
        "\n",
        "        if matches:\n",
        "            # Get the last matched answer and parse it as JSON\n",
        "            answer_json = json.loads(matches[-1].strip())\n",
        "            return answer_json['query']\n",
        "        else:\n",
        "            raise ValueError(\"No answer tags found in response\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to extract query from response: {e}\")\n",
        "\n",
        "def rewrite_query(query: str, api_url: str = \"http://localhost:8000/v1/chat/completions\") -> str:\n",
        "    \"\"\"Send the query to the vLLM API and get the rewritten version.\"\"\"\n",
        "    messages = format_prompt(query)\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"DeepRetrieval/DeepRetrieval-NQ-BM25-3B\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 512\n",
        "    }\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "\n",
        "        # Extract the generated text from the response\n",
        "        generated_text = result['choices'][0]['message']['content']\n",
        "\n",
        "        # Extract the rewritten query\n",
        "        rewritten_query = extract_query(generated_text)\n",
        "        return rewritten_query\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise Exception(f\"API request failed: {e}\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to process response: {e}\")\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Query rewriting using vLLM API\")\n",
        "    parser.add_argument(\"--query\", type=str, required=True, help=\"The query to rewrite\")\n",
        "    parser.add_argument(\"--api_url\", type=str, default=\"http://localhost:8000/v1/chat/completions\",\n",
        "                      help=\"URL of the vLLM API server (default: http://localhost:8000/v1/chat/completions)\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        rewritten_query = rewrite_query(args.query, args.api_url)\n",
        "        end_time = time.time()\n",
        "        print(f\"Original query: {args.query}\")\n",
        "        print(f\"Rewritten query: {rewritten_query}\")\n",
        "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "4M-So2ltzEQr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}